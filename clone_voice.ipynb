{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/suno-ai/bark.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBTrLJr6Yrrj",
        "outputId": "06cd7842-cc2b-46cc-b3ff-d97ba06ea298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/suno-ai/bark.git\n",
            "  Cloning https://github.com/suno-ai/bark.git to /tmp/pip-req-build-82_e03zq\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/suno-ai/bark.git /tmp/pip-req-build-82_e03zq\n",
            "  Resolved https://github.com/suno-ai/bark.git to commit 773624d26db84278a55aacae9a16d7b25fbccab8\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting boto3 (from suno-bark==0.0.1a0)\n",
            "  Downloading boto3-1.34.64-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting encodec (from suno-bark==0.0.1a0)\n",
            "  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting funcy (from suno-bark==0.0.1a0)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from suno-bark==0.0.1a0) (0.20.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from suno-bark==0.0.1a0) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from suno-bark==0.0.1a0) (1.11.4)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from suno-bark==0.0.1a0) (0.15.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from suno-bark==0.0.1a0) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from suno-bark==0.0.1a0) (4.66.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from suno-bark==0.0.1a0) (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (24.0)\n",
            "Collecting botocore<1.35.0,>=1.34.64 (from boto3->suno-bark==0.0.1a0)\n",
            "  Downloading botocore-1.34.64-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->suno-bark==0.0.1a0)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->suno-bark==0.0.1a0)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from encodec->suno-bark==0.0.1a0) (2.2.1+cu121)\n",
            "Collecting einops (from encodec->suno-bark==0.0.1a0)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->suno-bark==0.0.1a0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->suno-bark==0.0.1a0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->suno-bark==0.0.1a0) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->suno-bark==0.0.1a0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->suno-bark==0.0.1a0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->suno-bark==0.0.1a0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->suno-bark==0.0.1a0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->suno-bark==0.0.1a0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->suno-bark==0.0.1a0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch->suno-bark==0.0.1a0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->suno-bark==0.0.1a0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->suno-bark==0.0.1a0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch->suno-bark==0.0.1a0)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->suno-bark==0.0.1a0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->suno-bark==0.0.1a0) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->suno-bark==0.0.1a0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->suno-bark==0.0.1a0) (2023.12.25)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->suno-bark==0.0.1a0) (0.4.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.64->boto3->suno-bark==0.0.1a0) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.64->boto3->suno-bark==0.0.1a0) (2.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->suno-bark==0.0.1a0) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.14.1->suno-bark==0.0.1a0) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->suno-bark==0.0.1a0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.64->boto3->suno-bark==0.0.1a0) (1.16.0)\n",
            "Building wheels for collected packages: suno-bark, encodec\n",
            "  Building wheel for suno-bark (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for suno-bark: filename=suno_bark-0.0.1a0-py3-none-any.whl size=2567413 sha256=2d1f1399efd0c4f515a7627d4062063cf57fa8c3f04b9a18ab329ae64f9874ed\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mei1yhe5/wheels/e6/6d/c2/107ed849afe600f905bb4049a026df3c7c5aa75d86c2721ec7\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45762 sha256=ede18408ee0f38aa1a590bc4ab876657484084ade707dd1b2359301aa46b9328\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/36/cb/81af8b985a5f5e0815312d5e52b41263237af07b977e6bcbf3\n",
            "Successfully built suno-bark encodec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EFQS8WqX-l8"
      },
      "outputs": [],
      "source": [
        "from bark.generation import load_codec_model, generate_text_semantic\n",
        "from encodec.utils import convert_audio\n",
        "\n",
        "import torchaudio\n",
        "import torch\n",
        "\n",
        "device = 'cuda' # or 'cpu'\n",
        "model = load_codec_model(use_gpu=True if device == 'cuda' else False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/gitmylo/bark-voice-cloning-HuBERT-quantizer/\n",
        "%cd bark-voice-cloning-HuBERT-quantizer"
      ],
      "metadata": {
        "id": "_FaosMwBaeTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -r requirements.txt\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
      ],
      "metadata": {
        "id": "O_BmuG-7andN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JIdhrXOX-mA"
      },
      "outputs": [],
      "source": [
        "# From https://github.com/gitmylo/bark-voice-cloning-HuBERT-quantizer\n",
        "from hubert.hubert_manager import HuBERTManager\n",
        "hubert_manager = HuBERTManager()\n",
        "hubert_manager.make_sure_hubert_installed()\n",
        "hubert_manager.make_sure_tokenizer_installed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbQzJSv8X-mA"
      },
      "outputs": [],
      "source": [
        "# From https://github.com/gitmylo/bark-voice-cloning-HuBERT-quantizer\n",
        "# Load HuBERT for semantic tokens\n",
        "from hubert.pre_kmeans_hubert import CustomHubert\n",
        "from hubert.customtokenizer import CustomTokenizer\n",
        "\n",
        "# Load the HuBERT model\n",
        "hubert_model = CustomHubert(checkpoint_path='data/models/hubert/hubert.pt').to(device)\n",
        "\n",
        "# Load the CustomTokenizer model\n",
        "tokenizer = CustomTokenizer.load_from_checkpoint('data/models/hubert/tokenizer.pth').to(device)  # Automatically uses the right layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiagSSlAX-mB"
      },
      "outputs": [],
      "source": [
        "# Load and pre-process the audio waveform\n",
        "audio_filepath = 'audio.wav' # the audio you want to clone (under 13 seconds)\n",
        "wav, sr = torchaudio.load(audio_filepath)\n",
        "wav = convert_audio(wav, sr, model.sample_rate, model.channels)\n",
        "wav = wav.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTcYamh4X-mC"
      },
      "outputs": [],
      "source": [
        "semantic_vectors = hubert_model.forward(wav, input_sample_hz=model.sample_rate)\n",
        "semantic_tokens = tokenizer.get_token(semantic_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhloNKMdX-mC"
      },
      "outputs": [],
      "source": [
        "# Extract discrete codes from EnCodec\n",
        "with torch.no_grad():\n",
        "    encoded_frames = model.encode(wav.unsqueeze(0))\n",
        "codes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1).squeeze()  # [n_q, T]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUasx0WkX-mC"
      },
      "outputs": [],
      "source": [
        "# move codes to cpu\n",
        "codes = codes.cpu().numpy()\n",
        "# move semantic tokens to cpu\n",
        "semantic_tokens = semantic_tokens.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8bYgI6zX-mD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "voice_name = 'output' # whatever you want the name of the voice to be\n",
        "output_path = 'bark/assets/prompts/' + voice_name + '.npz'\n",
        "np.savez(output_path, fine_prompt=codes, coarse_prompt=codes[:2, :], semantic_prompt=semantic_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfzZqU_pX-mD"
      },
      "outputs": [],
      "source": [
        "# That's it! Now you can head over to the generate.ipynb and use your voice_name for the 'history_prompt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJxvfsGsX-mE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEJ_1h9WX-mE"
      },
      "outputs": [],
      "source": [
        "# Heres the generation stuff copy-pasted for convenience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bBRNxwDX-mE"
      },
      "outputs": [],
      "source": [
        "from bark.api import generate_audio\n",
        "from transformers import BertTokenizer\n",
        "from bark.generation import SAMPLE_RATE, preload_models, codec_decode, generate_coarse, generate_fine, generate_text_semantic\n",
        "\n",
        "# Enter your prompt and speaker here\n",
        "text_prompt = \"Hello, my name is Serpy. And, uh — and I like pizza. [laughs]\"\n",
        "voice_name = \"output\" # use your custom voice name here if you have one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbgaRkw-X-mE"
      },
      "outputs": [],
      "source": [
        "# download and load all models\n",
        "preload_models(\n",
        "    text_use_gpu=True,\n",
        "    text_use_small=False,\n",
        "    coarse_use_gpu=True,\n",
        "    coarse_use_small=False,\n",
        "    fine_use_gpu=True,\n",
        "    fine_use_small=False,\n",
        "    codec_use_gpu=True,\n",
        "    force_reload=False,\n",
        "    path=\"models\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WLlPWVAX-mF"
      },
      "outputs": [],
      "source": [
        "# simple generation\n",
        "audio_array = generate_audio(text_prompt, history_prompt=voice_name, text_temp=0.7, waveform_temp=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDpSWJcKX-mF"
      },
      "outputs": [],
      "source": [
        "# generation with more control\n",
        "x_semantic = generate_text_semantic(\n",
        "    text_prompt,\n",
        "    history_prompt=voice_name,\n",
        "    temp=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        ")\n",
        "\n",
        "x_coarse_gen = generate_coarse(\n",
        "    x_semantic,\n",
        "    history_prompt=voice_name,\n",
        "    temp=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        ")\n",
        "x_fine_gen = generate_fine(\n",
        "    x_coarse_gen,\n",
        "    history_prompt=voice_name,\n",
        "    temp=0.5,\n",
        ")\n",
        "audio_array = codec_decode(x_fine_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u6r42zwX-mF"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "# play audio\n",
        "Audio(audio_array, rate=SAMPLE_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnGudOD3X-mF"
      },
      "outputs": [],
      "source": [
        "from scipy.io.wavfile import write as write_wav\n",
        "# save audio\n",
        "filepath = \"/output/audio.wav\" # change this to your desired output path\n",
        "write_wav(filepath, SAMPLE_RATE, audio_array)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}